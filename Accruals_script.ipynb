{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we have to perform a lot of wrangling on the reports generated by Workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl as oxl\n",
    "from subprocess import call\n",
    "from os import remove, chdir, path, getcwd\n",
    "from io import BytesIO\n",
    "from sys import argv\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: move this to .settings to add clarity for future editor\n",
    "\n",
    "WD_report_name = \"EXP031-RPT-Process-Accruals_with_Expense_Report.xlsx\"\n",
    "WD2_report_name = \"EXP032-RPT-Process-Accruals-_No_Expense.xlsx\"\n",
    "wrangled_WD_report_name = \"EXP031_plus_EXP032_wrangled.csv\"\n",
    "wrangled_WD2_report_name = \"EXP032_wrangled.csv\"\n",
    "MF_JE_template_name = \"MF_JE_template.xlsx\"\n",
    "master_file_name = \"WD_Accruals_Master.xlsx\"\n",
    "input_folder_path = \"../Script/Input/\"\n",
    "generic_GL_account = 46540000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_vbs_script():\n",
    "    # this VBS script converts XLSX files to CSV format for faster processing\n",
    "    vbscript = \"\"\"if WScript.Arguments.Count < 3 Then\n",
    "        WScript.Echo \"Please specify the source and the destination files. Usage: ExcelToCsv <xls/xlsx source file> <csv destination file> <worksheet number (starts at 1)>\"\n",
    "        Wscript.Quit\n",
    "    End If\n",
    "\n",
    "    csv_format = 6\n",
    "\n",
    "    Set objFSO = CreateObject(\"Scripting.FileSystemObject\")\n",
    "\n",
    "    src_file = objFSO.GetAbsolutePathName(Wscript.Arguments.Item(0))\n",
    "    dest_file = objFSO.GetAbsolutePathName(WScript.Arguments.Item(1))\n",
    "    worksheet_number = CInt(WScript.Arguments.Item(2))\n",
    "\n",
    "    Dim oExcel\n",
    "    Set oExcel = CreateObject(\"Excel.Application\")\n",
    "\n",
    "    Dim oBook   \n",
    "    Set oBook = oExcel.Workbooks.Open(src_file)\n",
    "    oBook.Worksheets(worksheet_number).Activate\n",
    "\n",
    "    oBook.SaveAs dest_file, csv_format\n",
    "\n",
    "    oBook.Close False\n",
    "    oExcel.Quit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"ExcelToCsv.vbs\", \"wb\") as f:\n",
    "            f.write(vbscript.encode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e.msg)\n",
    "        print(\"VBS script for converting xlsx files to csv could not be generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(xlsx_file_path, has_sheets=False, skiprows=None, usecols=None):\n",
    "    # this function maps the generate_vbs_script() function to the input XLSX file\n",
    "    if has_sheets:\n",
    "        # sheet numbers to use; using the first three and I don't know how to retrieve no of sheets, hence the fixed numbers\n",
    "        sheets = map(str, range(1, 3))\n",
    "        sheet_dataframes = []\n",
    "        for sheet in sheets:\n",
    "            csv_file_path = \"../Script/{}{}{}\".format(input_folder_path, sheet, \".csv\")\n",
    "            call([\"cscript.exe\", \"../Script/ExcelToCsv.vbs\", xlsx_file_path, csv_file_path, sheet, r\"//B\"])\n",
    "            try:\n",
    "                sheet_dataframe = pd.read_csv(csv_file_path, encoding=\"latin-1\", engine=\"c\", usecols=usecols)\n",
    "            except Exception as e:\n",
    "                print(e.msg)\n",
    "                print(\"Sheets could not be converted to CSV format.\")\n",
    "            sheet_dataframes.append(sheet_dataframe)\n",
    "        return tuple(sheet_dataframes)\n",
    "    else:\n",
    "        csv_file_path = \"{}{}\".format(xlsx_file_path[:-4], \"csv\")\n",
    "        # //B is for batch mode; this is to avoid spam on the console :)\n",
    "        call([\"cscript.exe\", \"../Script/ExcelToCsv.vbs\", xlsx_file_path, csv_file_path, str(1), r\"//B\"])\n",
    "        if skiprows:\n",
    "            try:\n",
    "                data = pd.read_csv(csv_file_path, skiprows=skiprows, encoding=\"latin-1\", engine=\"c\", usecols=usecols)\n",
    "            except Exception as e:\n",
    "                print(e.msg)\n",
    "                print(\"Something went wrong... make sure report names weren't changed or debug the load_csv function\")\n",
    "        else:\n",
    "            try:\n",
    "                    data = pd.read_csv(csv_file_path, encoding=\"latin-1\", engine=\"c\", usecols=usecols)\n",
    "            except Exception as e:\n",
    "                print(e.msg)\n",
    "                print(\"Something went wrong... make sure report names weren't changed or debug the load_csv function\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all():\n",
    "    file_names = [WD_report_name, WD2_report_name, MF_JE_template_name, master_file_name]\n",
    "    dataframes = []\n",
    "    WD1_required_cols = [\"Entity Code\", \"Cost Center\", \"Expense Report Number\", \"Expense Item\", \"Net Amount LC\"]\n",
    "    WD2_required_cols = [\"Transaction ID\", \"Billing Amount\", \"Currency\", \"Report Cost Location\"]\n",
    "\n",
    "    # the script will be used by load_csv() to convert XLSX to CSV for faster processing\n",
    "    generate_vbs_script()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        if file_name == WD_report_name:\n",
    "            usecols = WD1_required_cols\n",
    "            skiprows = [0]\n",
    "        elif file_name == WD2_report_name:\n",
    "            usecols = WD2_required_cols\n",
    "        elif file_name == MF_JE_template_name:\n",
    "            with open(\"{}{}\".format(input_folder_path, MF_JE_template_name), \"rb\") as f:\n",
    "                in_mem_file = BytesIO(f.read())\n",
    "            MF_JE_template = oxl.load_workbook(in_mem_file)\n",
    "            dataframes.append(MF_JE_template)\n",
    "            continue\n",
    "        else:\n",
    "            # this will produce two CSVs from the two first two sheets of WD_Accruals_Master.xlsm\n",
    "            cc_to_ba, accounts = load_csv(\"{}{}\".format(input_folder_path, file_name), has_sheets=True)\n",
    "            dataframes.extend([cc_to_ba, accounts])\n",
    "            break\n",
    "        df = load_csv(\"{}{}\".format(input_folder_path, file_name), skiprows=skiprows, usecols=usecols)\n",
    "        dataframes.append(df)\n",
    "        # resetting params\n",
    "        usecols = skiprows = None\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_garbage():\n",
    "    # remove no longer needed files\n",
    "    WD_report_byproduct = \"{}{}\".format(WD_report_name[:-5], \".csv\")\n",
    "    WD2_report_byproduct = \"{}{}\".format(WD2_report_name[:-5], \".csv\")\n",
    "    excel_to_csv_macro_byproducts = [\"1.csv\", \"2.csv\", WD_report_byproduct, WD2_report_byproduct]\n",
    "    for byproduct in excel_to_csv_macro_byproducts:\n",
    "        remove(\"{}{}\".format(input_folder_path, byproduct))\n",
    "    remove(\"ExcelToCsv.vbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_cleanup():\n",
    "    # TODO: deal with scientific-notation-like business areas converting to sci-notation\n",
    "    global WD_report, WD2_report\n",
    "\n",
    "    collect_garbage()\n",
    "\n",
    "    # remove rows with total amount 0 or less / unfortunately, pandas nor Python are able to convert amounts in the format:\n",
    "    # 123,456.00 to float, hence need to either use localization (bad idea as the process is WW), or use below workaround\n",
    "    try:\n",
    "        WD_report[\"Net Amount LC\"] = WD_report[\"Net Amount LC\"].apply(lambda x: x.replace(\",\", \"\") if type(x) != float else x)\n",
    "    except:\n",
    "        pass\n",
    "    WD_report[\"Net Amount LC\"] = WD_report[\"Net Amount LC\"].map(float)\n",
    "    WD_report = WD_report[WD_report[\"Net Amount LC\"] > 0]\n",
    "    try:\n",
    "        WD2_report[\"Billing Amount\"] = WD2_report[\"Billing Amount\"].apply(lambda x: x.replace(\",\", \"\") if type(x) != float else x)\n",
    "    except:\n",
    "        pass\n",
    "    # for card expenses, negative amounts are put in parentheses, e.g. (100.00); below line removes lines with such amounts\n",
    "    WD2_report = WD2_report[WD2_report[\"Billing Amount\"].apply(lambda x: \"(\" not in x)]\n",
    "    WD2_report[\"Billing Amount\"] = WD2_report[\"Billing Amount\"].map(float)\n",
    "    # filer out lines with missing cost center/cost location, as this data is critical to generating an accrual\n",
    "    WD_report.dropna(subset=[\"Cost Center\"], inplace=True)\n",
    "    WD2_report.dropna(subset=[\"Report Cost Location\"], inplace=True)\n",
    "    # delete the duplicate cost centers/descriptions inside Cost Center/Cost Location column\n",
    "    WD_report[\"Cost Center\"] = WD_report[\"Cost Center\"].astype(\"str\").map(lambda x: x.split()[0])\n",
    "    WD2_report[\"Report Cost Location\"] = WD2_report[\"Report Cost Location\"].astype(\"str\").map(lambda x: x.split()[0])\n",
    "    # add \"Company code\" column as it will be used by generate_output() to generate a separate folder for each company code\n",
    "    WD2_report[\"Company code\"] = WD2_report[\"Report Cost Location\"].apply(lambda x: x[:4])\n",
    "    WD_report = WD_report[WD_report[\"Expense Report Number\"].apply(lambda x: \"Cancelled\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vlookup(report, what, left_on, right_on):\n",
    "    merged = report.merge(what, left_on=left_on, right_on=right_on, how=\"left\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_vlookups():\n",
    "    global WD_report, WD2_report\n",
    "    accounts = pd.DataFrame(accounts_file[\"Account\"]).astype(int)\n",
    "    master_data_to_join = master_data_file[[\"Business Area\", \"Profit Center\", \"MRU\", \"Functional Area\"]]\n",
    "\n",
    "    WD_report = vlookup(WD_report, accounts, left_on=[WD_report[\"Expense Item\"], WD_report[\"Entity Code\"]], right_on=[accounts_file[\"Expense Item name\"], accounts_file[\"Subsidiary\"]])\n",
    "    # the account number is provided separately for each country. However, all countries have the same account for a given category, so we need to remove these duplicate rows.\n",
    "    # in case any country has a separate account for a given category in the future, the script will still work\n",
    "    WD_report = vlookup(WD_report, master_data_to_join, WD_report[\"Cost Center\"], master_data_file[\"Cost Center\"])\n",
    "    WD2_report = vlookup(WD2_report, master_data_to_join, WD2_report[\"Report Cost Location\"], master_data_file[\"Cost Center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_cleanup():\n",
    "    global WD_report\n",
    "    global accounts_file\n",
    "    travel_journal_item_account = 46540000\n",
    "    company_celebration_account = 46900000\n",
    "    german_debit_account = 46920000\n",
    "\n",
    "    # add vlookup exceptions\n",
    "    no_of_items = WD_report.shape[0]\n",
    "    for row_index in range(no_of_items):\n",
    "        category = str(WD_report[\"Expense Item\"].iloc[row_index])  # for some reason this column is loaded as float, hence the str()\n",
    "        if \"Travel Journal Item\" in category:\n",
    "            WD_report.loc[row_index, \"Account\"] = travel_journal_item_account\n",
    "            # WD_report.set_value(index, \"Acc#\", travel_journal_item_account)\n",
    "        if \"Company Celebration\" in category:\n",
    "            # WD_report.set_value(index, \"Acc#\", company_celebration_account)\n",
    "            WD_report.loc[row_index, \"Account\"] = company_celebration_account\n",
    "\n",
    "    # controllership requirement: change all 9999 BA's to 1019, 1059 to 1015\n",
    "    WD_report[\"Business Area\"] = WD_report[\"Business Area\"].apply(lambda x: \"1019\" if str(x) == \"9999\" else x)\n",
    "    WD_report[\"Business Area\"] = WD_report[\"Business Area\"].apply(lambda x: \"1015\" if str(x) == \"1059\" else x)\n",
    "\n",
    "    # this is to stop Excel from reading e.g. 2E00 as a number in scientific notation\n",
    "    WD_report[\"Business Area\"] = WD_report[\"Business Area\"].map(str)\n",
    "\n",
    "    # note that this also overrides the above two exceptions, which are changed to the german account\n",
    "    WD_report.loc[WD_report[\"Entity Code\"] == \"DESA\", \"Account\"] = german_debit_account\n",
    "\n",
    "    # ensure that account number is provided, and that it is an integer\n",
    "    try:\n",
    "        WD_report[\"Account\"] = WD_report[\"Account\"].map(int)\n",
    "    except:\n",
    "        # this means that some account numbers were not found for a company code-category combination -> use an account\n",
    "        # for the same category, but another company code (all CCs should use the same account)\n",
    "        lines_with_missing_account = WD_report[WD_report[\"Account\"].isnull()]\n",
    "        # remove above lines from WD_report\n",
    "        WD_report = WD_report[~WD_report[\"Account\"].isnull()]\n",
    "        # remove duplicate categories, effectively leaving the first found acc # for a given category, which is what is going to be assigned for missing values\n",
    "        deduplicated_accounts_file = accounts_file.drop_duplicates(subset=[\"Expense Item name\"])\n",
    "        accounts = pd.DataFrame(deduplicated_accounts_file[\"Account\"])\n",
    "        # dropping Account column so that merge does not produce useless new columns\n",
    "        #deduplicated_accounts_file.drop(\"Account\", axis=1, inplace=True)\n",
    "        #accounts.rename(columns={\"Account\": \"Acc#\"}, inplace=True)\n",
    "        lines_with_missing_account.drop(\"Account\", axis=1, inplace=True)\n",
    "        merged = lines_with_missing_account.merge(accounts, left_on=lines_with_missing_account[\"Expense Item\"],\n",
    "                                                  right_on=deduplicated_accounts_file[\"Expense Item name\"], how=\"left\")\n",
    "        WD_report = WD_report.append(merged)\n",
    "        WD_report[\"Account\"] = WD_report[\"Account\"].map(int)\n",
    "\n",
    "    # add a checksum so we can group by BA + PC combinations\n",
    "    WD_report[\"Checksum\"] = WD_report[\"Profit Center\"].astype(str) + WD_report[\"Business Area\"]\n",
    "    WD2_report[\"Checksum\"] = WD2_report[\"Profit Center\"].astype(str) + WD2_report[\"Business Area\"]\n",
    "    \n",
    "    # restore column order after df.append()\n",
    "    final_column_order = [\"Entity Code\", \"Cost Center\", \"Expense Report Number\", \"Expense Item\", \"Net Amount LC\", \"Account\", \"Business Area\", \"Profit Center\", \"Functional Area\", \"MRU\", \"Checksum\"]\n",
    "    WD_report = WD_report.reindex(columns=final_column_order)\n",
    "    \n",
    "    # add the generic account number to all card expenses\n",
    "    WD2_report[\"Account\"] = generic_GL_account\n",
    "    \n",
    "    # drop currency column, rename WD2's columns to match WD's, and append it\n",
    "    WD1_to_WD2_columns_mapping = {\"Transaction ID\": \"Expense Report Number\", \"Billing Amount\": \"Net Amount LC\", \n",
    "                                  \"Report Cost Location\": \"Cost Center\", \"Company code\": \"Entity Code\"}\n",
    "    WD2_report.drop(\"Currency\", axis=1, inplace=True)\n",
    "    WD2_report.rename(columns=WD1_to_WD2_columns_mapping, inplace=True)\n",
    "    WD_report = WD_report.append(WD2_report)\n",
    "    \n",
    "    # rename the column with card transaction numbers back to a senslible name\n",
    "    WD2_report.rename(columns={\"Expense Report Number\": \"Transaction number\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add below cell to the final code; ensure getcwd() return correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chdir(path.dirname(argv[0]))\n",
    "# getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\syf\\Anaconda\\lib\\site-packages\\openpyxl\\reader\\worksheet.py:310: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "C:\\syf\\Anaconda\\lib\\site-packages\\openpyxl\\reader\\worksheet.py:310: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "WD_report, WD2_report, MF_JE_template, master_data_file, accounts_file = load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_vlookups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrangled_WD_report_save_path = \"../Script/Output/wrangled_reports/\" + wrangled_WD_report_name\n",
    "wrangled_WD2_report_save_path = \"../Script/Output/wrangled_reports/\" + wrangled_WD2_report_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WD_report.to_csv(wrangled_WD_report_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WD2_report.to_csv(wrangled_WD2_report_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use these wrangled files to generate CSVs in the format accepted by Netsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrangled_WD_report = pd.read_csv(\"../Script/Output/wrangled_reports/{}\".format(wrangled_WD_report_name))\n",
    "WD_report_groupby_input = wrangled_WD_report[[\"Entity Code\", \"Checksum\", \"Account\", \"Expense Report Number\", \"Net Amount LC\", \"MRU\", \"Functional Area\"]]\n",
    "grouped_by_cc = WD_report_groupby_input.groupby(\"Entity Code\", as_index=False)\n",
    "JE_csv_columns = [\"ACCOUNT\", \"DEBIT\", \"CREDIT\", \"TAX CODE\", \"LINE MEMO\", \"MRU\", \"BUSINESS AREA\", \"PROFIT CENTER\", \"FUNCTIONAL AREA\",\n",
    "                  \"DATE\", \"POSTING PERIOD\", \"ACCOUNTING BOOK\", \"SUBSIDIARY\", \"CURRENCY\", \"MEMO\", \"REVERSAL DATE\", \"TO SUBSIDIARY\",\n",
    "                  \"TRADING PARTNER\", \"TRADING PARTNER CODE\", \"UNIQUE ID\"]\n",
    "last_day_of_previous_month = pd.to_datetime(\"today\") - pd.tseries.offsets.MonthEnd(1)\n",
    "date_cut = last_day_of_previous_month.strftime(\"%m.%y\")\n",
    "first_day_of_current_month = pd.to_datetime(\"today\").replace(day=1).strftime(\"%m/%d/%Y\")\n",
    "AP_account = 25702400  # the account from which the money will flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_exchange_rates():\n",
    "    # https://openexchangerates.org\n",
    "    exchange_rates_api_key = \"11f20df062814531be891cc0173702a6\"\n",
    "    api_call = f\"https://openexchangerates.org/api/latest.json?app_id={exchange_rates_api_key}\"\n",
    "    rates_api_response = urlopen(api_call)\n",
    "    rates_api_response_str = rates_api_response.read().decode(\"ascii\")\n",
    "    rates_api_response_dict = loads(rates_api_response_str)\n",
    "    rates = rates_api_response_dict[\"rates\"]\n",
    "\n",
    "    # feel free to update company codes/currencies\n",
    "    currencies_in_scope = {\"AUSA\": \"AUD\", \"BESA\": \"EUR\", \"BGSA\": \"BGN\", \"BRSA\": \"BRL\", \"CASA\": \"CAD\", \"CHSD\": \"CHF\", \"CNSA\": \"CNY\",\n",
    "                           \"CRSB\": \"CRC\", \"CZSA\": \"CZK\", \"DESA\": \"EUR\", \"DKSA\": \"DKK\", \"ESSA\": \"EUR\", \"FRSA\": \"EUR\", \"GBF0\": \"USD\",\n",
    "                           \"GBSA\": \"GBP\", \"IESA\": \"EUR\", \"IESB\": \"EUR\", \"ILSA\": \"ILS\", \"ILSB\": \"ILS\", \"INSA\": \"INR\", \"INSB\": \"INR\",\n",
    "                           \"INSD\": \"INR\", \"ITSA\": \"EUR\", \"JPSA\": \"JPY\", \"LUSB\": \"EUR\", \"MXSC\": \"MXN\", \"NLSC\": \"EUR\", \"PHSB\": \"PHP\",\n",
    "                           \"PLSA\": \"PLN\", \"PRSA\": \"PYG\", \"ROSA\": \"RON\", \"RUSA\": \"RUB\", \"SESA\": \"SEK\", \"TRSA\": \"TRY\", \"USMS\": \"USD\",\n",
    "                           \"USSM\": \"USD\", \"USSN\": \"USD\"}\n",
    "\n",
    "    exchange_rates_to_usd = {}\n",
    "    for company_code in currencies_in_scope:\n",
    "        currency = currencies_in_scope[company_code]\n",
    "        # the rates from API are from USD to x; we need from x to USD\n",
    "        try:\n",
    "            exchange_rate_to_usd = 1/rates[currency]\n",
    "        except:\n",
    "            continue\n",
    "        if company_code in exchange_rates_to_usd:\n",
    "            continue\n",
    "        else:\n",
    "            exchange_rates_to_usd[company_code] = exchange_rate_to_usd\n",
    "    return exchange_rates_to_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(cc):  # cc = Company Code\n",
    "\n",
    "    CSV_file_path = \"../Script/Output/upload_to_Netsuite/{}_Accrual_WD_{}.csv\".format(cc, date_cut)\n",
    "    MF_JE_template_save_path = \"../Script/Output/upload_to_Sharepoint/{}_{}{}\".format(cc, MF_JE_template_name[:-5], \".xlsx\")\n",
    "    JE_csv = pd.DataFrame(columns=JE_csv_columns)\n",
    "    cur_cc_data = grouped_by_cc.get_group(cc)\n",
    "    grouped_by_checksum = cur_cc_data.groupby([\"Checksum\"])\n",
    "    posting_month = last_day_of_previous_month.strftime(\"%b\")\n",
    "    posting_year = last_day_of_previous_month.strftime(\"%Y\")\n",
    "    posting_period = \"{} {}\".format(posting_month, posting_year)\n",
    "    # this is a way to track row number, so that groups can be input to consecutive rows\n",
    "    cur_group_start_row = 0\n",
    "\n",
    "    for checksum, g in grouped_by_checksum:\n",
    "        business_area = checksum[-4:]  # BA is the last 4 chars of checksum\n",
    "        profit_center = checksum[:5]  # PC is the first 5 chars of checksum\n",
    "        general_description = \"WD {} ACCRUALS {} FY{}\".format(cc, posting_month, posting_year)\n",
    "        for i in range(cur_group_start_row, cur_group_start_row + len(g)):\n",
    "            # for each line for a given checksum (BA and PC combination), retrieve its Acc# culumn value and input it\n",
    "            # into the next free cell in the \"ACCOUNT\" column in the JE csv form\n",
    "            JE_csv.loc[i, \"ACCOUNT\"] = g.iloc[i - cur_group_start_row][\"Account\"]\n",
    "            JE_csv.loc[i, \"DEBIT\"] = g.iloc[i - cur_group_start_row][\"Net Amount LC\"]\n",
    "            JE_csv.loc[i, \"LINE MEMO\"] = g.iloc[i - cur_group_start_row][\"Expense Report Number\"] + \" Accrual\"\n",
    "            # Note that even though the template has a TRANSACTION DATE - DAY field, it still passes the whole date in mm/dd/YYYY format\n",
    "            JE_csv.loc[i, \"DATE\"] = last_day_of_previous_month.strftime(\"%m/%d/%Y\")\n",
    "            JE_csv.loc[i, \"POSTING PERIOD\"] = posting_period\n",
    "            JE_csv.loc[i, \"SUBSIDIARY\"] = cc\n",
    "            JE_csv.loc[i, \"MEMO\"] = general_description\n",
    "            JE_csv.loc[i, \"REVERSAL DATE\"] = first_day_of_current_month\n",
    "            JE_csv.loc[i, \"MRU\"] = g.iloc[i - cur_group_start_row][\"MRU\"]\n",
    "            JE_csv.loc[i, \"FUNCTIONAL AREA\"] = g.iloc[i - cur_group_start_row][\"Functional Area\"]\n",
    "\n",
    "        # here we're filling out the AP account row\n",
    "        last_group_start_row = cur_group_start_row\n",
    "        cur_group_start_row += len(g)\n",
    "        JE_csv.loc[cur_group_start_row, \"ACCOUNT\"] = AP_account\n",
    "        JE_csv.loc[cur_group_start_row, \"CREDIT\"] = JE_csv.loc[last_group_start_row:cur_group_start_row, \"DEBIT\"].sum()\n",
    "        JE_csv.loc[cur_group_start_row, \"LINE MEMO\"] = general_description\n",
    "        JE_csv.loc[cur_group_start_row, \"BUSINESS AREA\"] = business_area\n",
    "        JE_csv.loc[cur_group_start_row, \"PROFIT CENTER\"] = profit_center\n",
    "        JE_csv.loc[cur_group_start_row, \"DATE\"] = last_day_of_previous_month.strftime(\"%m/%d/%Y\")\n",
    "        JE_csv.loc[cur_group_start_row, \"POSTING PERIOD\"] = posting_period\n",
    "        JE_csv.loc[cur_group_start_row, \"SUBSIDIARY\"] = cc\n",
    "        JE_csv.loc[cur_group_start_row, \"MEMO\"] = general_description\n",
    "        JE_csv.loc[cur_group_start_row, \"REVERSAL DATE\"] = first_day_of_current_month\n",
    "        cur_group_start_row += 1\n",
    "\n",
    "    JE_amount_local = JE_csv[\"CREDIT\"].sum(skipna=True)\n",
    "    exchange_rates = generate_exchange_rates()\n",
    "    amount_in_usd =  JE_amount_local * exchange_rates[cc]\n",
    "    to_generate = []\n",
    "\n",
    "    # company requirement\n",
    "    if amount_in_usd > 5000:\n",
    "        to_generate.append(cc)\n",
    "\n",
    "    if cc in to_generate:\n",
    "        JE_csv.to_csv(CSV_file_path, index=False)\n",
    "        print(\"{} CSV file generated :)\".format(cc))\n",
    "        # TODO: since wb.save() closes the workbook, we would need to reopen it on each loop... hence doing it \n",
    "        # with the deeper openpyxl.writer.excel.ExcelWriter's write_data() method\n",
    "        archive = ZipFile(MF_JE_template_save_path,'w', ZIP_DEFLATED, allowZip64=True)\n",
    "        writer = oxl.writer.excel.ExcelWriter(MF_JE_template, archive)\n",
    "        #writer._comments = [] TODO: do this for each sheet\n",
    "        writer.write_data()\n",
    "        #MF_JE_template.save(MF_JE_template_save_path)\n",
    "        print(\"{} template file generated :)\".format(cc))\n",
    "    MF_JE_template.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUSA CSV file generated :)\n",
      "AUSA template file generated :)\n",
      "DESA CSV file generated :)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-4f4d12b65ffe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouped_by_cc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcompany_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mgenerate_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-e123e78b7840>\u001b[0m in \u001b[0;36mgenerate_output\u001b[1;34m(cc)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moxl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMF_JE_template\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m#writer._comments = [] TODO: do this for each sheet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;31m#MF_JE_template.save(MF_JE_template_save_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} template file generated :)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\syf\\Anaconda\\lib\\site-packages\\openpyxl\\writer\\excel.py\u001b[0m in \u001b[0;36mwrite_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0marchive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritestr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mARC_THEME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_theme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_worksheets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_chartsheets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\syf\\Anaconda\\lib\\site-packages\\openpyxl\\writer\\excel.py\u001b[0m in \u001b[0;36m_write_worksheets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_comment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_drawing\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\syf\\Anaconda\\lib\\site-packages\\openpyxl\\writer\\excel.py\u001b[0m in \u001b[0;36m_write_comment\u001b[1;34m(self, ws)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mvml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mvml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvba_archive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_drawing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mvml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "for key, group in grouped_by_cc:\n",
    "    company_code = key\n",
    "    generate_output(company_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: generate backup template files using openpyxl:\n",
    "### load_MF_JE_template()\n",
    "### if cc in to_generate:\n",
    "        ### filled = fill_template(cc, template)\n",
    "        ### filled.save(path)\n",
    "        ### \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
