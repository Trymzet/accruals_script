{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we have to perform a lot of wrangling on the crappy reports generated by Workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "from subprocess import call\n",
    "from os import remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WD_report_name = \"EXP031-RPT-Process-Accruals_with_Expense_Report.xlsx\"\n",
    "WD2_report_name = \"EXP032-RPT-Process-Accruals-_No_Expense.xlsx\"\n",
    "wrangled_WD_report_name = \"EXP031_wrangled.csv\"\n",
    "wrangled_WD2_report_name = \"EXP032_wrangled.csv\"\n",
    "generic_GL_account = 46540000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_vbs_script():\n",
    "    # this VBS script converts XLSX files to CSV format for faster processing\n",
    "    vbscript = \"\"\"if WScript.Arguments.Count < 3 Then\n",
    "        WScript.Echo \"Please specify the source and the destination files. Usage: ExcelToCsv <xls/xlsx source file> <csv destination file> <worksheet number (starts at 1)>\"\n",
    "        Wscript.Quit\n",
    "    End If\n",
    "\n",
    "    csv_format = 6\n",
    "\n",
    "    Set objFSO = CreateObject(\"Scripting.FileSystemObject\")\n",
    "\n",
    "    src_file = objFSO.GetAbsolutePathName(Wscript.Arguments.Item(0))\n",
    "    dest_file = objFSO.GetAbsolutePathName(WScript.Arguments.Item(1))\n",
    "    worksheet_number = CInt(WScript.Arguments.Item(2))\n",
    "\n",
    "    Dim oExcel\n",
    "    Set oExcel = CreateObject(\"Excel.Application\")\n",
    "\n",
    "    Dim oBook   \n",
    "    Set oBook = oExcel.Workbooks.Open(src_file)\n",
    "    oBook.Worksheets(worksheet_number).Activate\n",
    "\n",
    "    oBook.SaveAs dest_file, csv_format\n",
    "\n",
    "    oBook.Close False\n",
    "    oExcel.Quit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"ExcelToCsv.vbs\", \"wb\") as f:\n",
    "            f.write(vbscript.encode(\"utf-8\"))\n",
    "    except:\n",
    "        print(\"VBS script for converting xlsx files to csv could not be generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(xlsx_file_name, has_sheets=False, skiprows=None, usecols=None):\n",
    "    # this function maps the generate_vbs_script() function to the input XLSX file\n",
    "    if has_sheets:\n",
    "        # sheet numbers to use; using the first three and I don't know how to retrieve no of sheets, hence the fixed numbers\n",
    "        sheets = map(str, range(1, 4))\n",
    "        sheet_dataframes = []\n",
    "        for sheet in sheets:\n",
    "            csv_file_name = \"../Script/{}{}\".format(sheet, \".csv\")\n",
    "            call([\"cscript.exe\", \"../Script/ExcelToCsv.vbs\", xlsx_file_name, csv_file_name, sheet, r\"//B\"])\n",
    "            try:\n",
    "                sheet_dataframe = pd.read_csv(csv_file_name, encoding=\"latin-1\", engine=\"c\", usecols=usecols)\n",
    "            except:\n",
    "                print(\"Sheets could not be converted to CSV format.\")\n",
    "            sheet_dataframes.append(sheet_dataframe)\n",
    "        return tuple(sheet_dataframes)\n",
    "    else:\n",
    "        csv_file_name = \"{}{}\".format(xlsx_file_name[:-4], \"csv\")\n",
    "        # //B is for batch mode; this is to avoid spam on the console :)\n",
    "        call([\"cscript.exe\", \"../Script/ExcelToCsv.vbs\", xlsx_file_name, csv_file_name, str(1), r\"//B\"])\n",
    "        if skiprows:\n",
    "            try:\n",
    "                data = pd.read_csv(csv_file_name, skiprows=skiprows, encoding=\"latin-1\", engine=\"c\", usecols=usecols)\n",
    "\n",
    "            except:\n",
    "                print(\"Something went wrong... make sure report names weren't changed or debug the load_csv function\")\n",
    "        else:\n",
    "            try:\n",
    "                    data = pd.read_csv(csv_file_name, encoding=\"latin-1\", engine=\"c\", usecols=usecols)\n",
    "            except:\n",
    "                print(\"Something went wrong... make sure report names weren't changed or debug the load_csv function\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all():\n",
    "    master_file_name = \"WD_Accruals_Master.xlsm\"\n",
    "    file_names = [WD_report_name, WD2_report_name, master_file_name]\n",
    "    dataframes = []\n",
    "    WD1_required_cols = [\"Entity Code\", \"Cost Center\", \"Expense Report Number\", \"Expense Item\", \"Net Amount LC\"]\n",
    "    WD2_required_cols = [\"Transaction ID\", \"Billing Amount\", \"Currency\", \"Report Cost Location\"]\n",
    "\n",
    "    # the script will be used by load_csv() to convert XLSX to CSV for faster processing\n",
    "    generate_vbs_script()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        if file_name == WD_report_name:\n",
    "            df = load_csv(file_name, skiprows=[0], usecols=WD1_required_cols)\n",
    "        elif file_name == WD2_report_name:\n",
    "            df = load_csv(file_name, usecols=WD2_required_cols)\n",
    "        else:\n",
    "            cc_to_ba, accounts, JE_template = load_csv(file_name, has_sheets=True)\n",
    "            dataframes.extend([cc_to_ba, accounts, JE_template])\n",
    "            return dataframes\n",
    "        dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_garbage():\n",
    "    # remove no longer needed files\n",
    "    WD_report_byproduct = \"{}{}\".format(WD_report_name[:-5], \".csv\")\n",
    "    WD2_report_byproduct = \"{}{}\".format(WD2_report_name[:-5], \".csv\")\n",
    "    excel_to_csv_macro_byproducts = [\"1.csv\", \"2.csv\", \"3.csv\", WD_report_byproduct, WD2_report_byproduct]\n",
    "    for byproduct in excel_to_csv_macro_byproducts:\n",
    "        remove(byproduct)\n",
    "    remove(\"ExcelToCsv.vbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_cleanup():\n",
    "    # TODO: deal with scientific-notation-like business areas converting to sci-notation\n",
    "    global WD_report, WD2_report\n",
    "\n",
    "    collect_garbage()\n",
    "\n",
    "    # remove rows with total amount 0 or less / unfortunately, pandas nor Python are able to convert amounts in the format:\n",
    "    # 123,456.00 to float, hence need to either use localization (bad idea as the process is WW), or use below workaround\n",
    "    try:\n",
    "        WD_report[\"Net Amount LC\"] = WD_report[\"Net Amount LC\"].apply(lambda x: x.replace(\",\", \"\") if type(x) != float else x)\n",
    "    except:\n",
    "        pass\n",
    "    WD_report[\"Net Amount LC\"] = WD_report[\"Net Amount LC\"].map(float)\n",
    "    WD_report = WD_report[WD_report[\"Net Amount LC\"] > 0]\n",
    "    try:\n",
    "        WD2_report[\"Billing Amount\"] = WD2_report[\"Billing Amount\"].apply(lambda x: x.replace(\",\", \"\") if type(x) != float else x)\n",
    "    except:\n",
    "        pass\n",
    "    # for card expenses, negative amounts are put in parentheses, e.g. (100.00); below line removes lines with such amounts\n",
    "    WD2_report = WD2_report[WD2_report[\"Billing Amount\"].apply(lambda x: \"(\" not in x)]\n",
    "    WD2_report[\"Billing Amount\"] = WD2_report[\"Billing Amount\"].map(float)\n",
    "    # filer out lines with missing cost center/cost location, as this data is critical to generating an accrual\n",
    "    WD_report.dropna(subset=[\"Cost Center\"], inplace=True)\n",
    "    WD2_report.dropna(subset=[\"Report Cost Location\"], inplace=True)\n",
    "    # delete the duplicate cost centers/descriptions inside Cost Center/Cost Location column\n",
    "    WD_report[\"Cost Center\"] = WD_report[\"Cost Center\"].astype(\"str\").map(lambda x: x.split()[0])\n",
    "    WD2_report[\"Report Cost Location\"] = WD2_report[\"Report Cost Location\"].astype(\"str\").map(lambda x: x.split()[0])\n",
    "    # add \"Company code\" column as it will be used by generate_csv() to generate a separate file for each company code\n",
    "    WD2_report[\"Company code\"] = WD2_report[\"Report Cost Location\"].apply(lambda x: x[:4])\n",
    "    WD_report = WD_report[WD_report[\"Expense Report Number\"].apply(lambda x: \"Cancelled\" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vlookup(report, what, left_on, right_on):\n",
    "    merged = report.merge(what, left_on=left_on, right_on=right_on, how=\"left\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_vlookups():\n",
    "    global WD_report, WD2_report\n",
    "    accounts = pd.DataFrame(accounts_file[\"Account\"]).astype(int)\n",
    "    master_data_to_join = master_data_file[[\"Business Area\", \"Profit Center\", \"MRU\", \"Functional Area\"]]\n",
    "\n",
    "    WD_report = vlookup(WD_report, accounts, left_on=[WD_report[\"Expense Item\"], WD_report[\"Entity Code\"]], right_on=[accounts_file[\"Expense Item name\"], accounts_file[\"Subsidiary\"]])\n",
    "    # the account number is provided separately for each country. However, all countries have the same account for a given category, so we need to remove these duplicate rows.\n",
    "    # in case any country has a separate account for a given category in the future, the script will still work\n",
    "    WD_report = vlookup(WD_report, master_data_to_join, WD_report[\"Cost Center\"], master_data_file[\"Cost Center\"])\n",
    "    WD2_report = vlookup(WD2_report, master_data_to_join, WD2_report[\"Report Cost Location\"], master_data_file[\"Cost Center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_cleanup():\n",
    "    global WD_report\n",
    "    global accounts_file\n",
    "    travel_journal_item_account = 46540000\n",
    "    company_celebration_account = 46900000\n",
    "    german_debit_account = 46920000\n",
    "\n",
    "    # add vlookup exceptions\n",
    "    no_of_items = WD_report.shape[0]\n",
    "    for row_index in range(no_of_items):\n",
    "        category = str(WD_report[\"Expense Item\"].iloc[row_index])  # for some reason this column is loaded as float, hence the str()\n",
    "        if \"Travel Journal Item\" in category:\n",
    "            WD_report.loc[row_index, \"Account\"] = travel_journal_item_account\n",
    "            # WD_report.set_value(index, \"Acc#\", travel_journal_item_account)\n",
    "        if \"Company Celebration\" in category:\n",
    "            # WD_report.set_value(index, \"Acc#\", company_celebration_account)\n",
    "            WD_report.loc[row_index, \"Account\"] = company_celebration_account\n",
    "\n",
    "    # controllership requirement: change all 9999 BA's to 1019, 1059 to 1015\n",
    "    WD_report[\"Business Area\"] = WD_report[\"Business Area\"].apply(lambda x: \"1019\" if str(x) == \"9999\" else x)\n",
    "    WD_report[\"Business Area\"] = WD_report[\"Business Area\"].apply(lambda x: \"1015\" if str(x) == \"1059\" else x)\n",
    "\n",
    "    # this is to stop Excel from reading e.g. 2E00 as a number in scientific notation\n",
    "    WD_report[\"Business Area\"] = WD_report[\"Business Area\"].map(str)\n",
    "\n",
    "    # note that this also overrides the above two exceptions, which are changed to the german account\n",
    "    WD_report.loc[WD_report[\"Entity Code\"] == \"DESA\", \"Account\"] = german_debit_account\n",
    "\n",
    "    # ensure that account number is provided, and that it is an integer\n",
    "    try:\n",
    "        WD_report[\"Account\"] = WD_report[\"Account\"].map(int)\n",
    "    except:\n",
    "        # this means that some account numbers were not found for a company code-category combination -> use an account\n",
    "        # for the same category, but another company code (all CCs should use the same account)\n",
    "        lines_with_missing_account = WD_report[WD_report[\"Account\"].isnull()]\n",
    "        # remove above lines from WD_report\n",
    "        WD_report = WD_report[~WD_report[\"Account\"].isnull()]\n",
    "        # remove duplicate categories, effectively leaving the first found acc # for a given category, which is what is going to be assigned for missing values\n",
    "        deduplicated_accounts_file = accounts_file.drop_duplicates(subset=[\"Expense Item name\"])\n",
    "        accounts = pd.DataFrame(deduplicated_accounts_file[\"Account\"])\n",
    "        # dropping Account column so that merge does not produce useless new columns\n",
    "        #deduplicated_accounts_file.drop(\"Account\", axis=1, inplace=True)\n",
    "        #accounts.rename(columns={\"Account\": \"Acc#\"}, inplace=True)\n",
    "        lines_with_missing_account.drop(\"Account\", axis=1, inplace=True)\n",
    "        merged = lines_with_missing_account.merge(accounts, left_on=lines_with_missing_account[\"Expense Item\"],\n",
    "                                                  right_on=deduplicated_accounts_file[\"Expense Item name\"], how=\"left\")\n",
    "        WD_report = WD_report.append(merged)\n",
    "        # append puts the Account column as the first one -- move it to the right\n",
    "        WD_report[\"Account\"] = WD_report[\"Account\"].map(int)\n",
    "\n",
    "    # add a checksum so we can group by BA + PC combinations\n",
    "    WD_report[\"Checksum\"] = WD_report[\"Profit Center\"].astype(str) + WD_report[\"Business Area\"]\n",
    "    WD2_report[\"Checksum\"] = WD2_report[\"Profit Center\"].astype(str) + WD2_report[\"Business Area\"]\n",
    "    #WD_report = WD_report[[c for c in WD_report if c != \"Account\"] + [\"Account\"]]\n",
    "    final_column_order = [\"Entity Code\", \"Cost Center\", \"Expense Report Number\", \"Expense Item\", \"Net Amount LC\", \"Account\", \"Business Area\", \"Profit Center\", \"Functional Area\", \"MRU\", \"Checksum\"]\n",
    "    WD_report = WD_report.reindex(columns=final_column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\syf\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2862: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "WD_report, WD2_report, master_data_file, accounts_file, JE_template_file = load_all()\n",
    "print(\"All files loaded :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_vlookups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrangled_WD_report_save_path = \"../Script/Output/wrangled_reports/\" + wrangled_WD_report_name\n",
    "wrangled_WD2_report_save_path = \"../Script/Output/wrangled_reports/\" + wrangled_WD2_report_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WD_report.to_csv(wrangled_WD_report_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "WD2_report.to_csv(wrangled_WD2_report_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use these wrangled files to generate CSVs in the format accepted by Netsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrangled_WD_report = pd.read_csv(\"../Script/Output/wrangled_reports/{}\".format(wrangled_WD_report_name))\n",
    "WD_report_groupby_input = wrangled_WD_report[[\"Entity Code\", \"Checksum\", \"Account\", \"Expense Report Number\", \"Net Amount LC\", \"MRU\", \"Functional Area\"]]\n",
    "grouped_by_cc = WD_report_groupby_input.groupby(\"Entity Code\", as_index=False)\n",
    "JE_csv_columns = [\"ACCOUNT\", \"DEBIT\", \"CREDIT\", \"TAX CODE\", \"LINE MEMO\", \"MRU\", \"BUSINESS AREA\", \"PROFIT CENTER\", \"FUNCTIONAL AREA\",\n",
    "                  \"DATE\", \"POSTING PERIOD\", \"ACCOUNTING BOOK\", \"SUBSIDIARY\", \"CURRENCY\", \"MEMO\", \"REVERSAL DATE\", \"TO SUBSIDIARY\",\n",
    "                  \"TRADING PARTNER\", \"TRADING PARTNER CODE\", \"UNIQUE ID\"]\n",
    "last_day_of_previous_month = pd.to_datetime(\"today\") - pd.tseries.offsets.MonthEnd(1)\n",
    "date_cut = last_day_of_previous_month.strftime(\"%m.%y\")\n",
    "first_day_of_current_month = pd.to_datetime(\"today\").replace(day=1).strftime(\"%m/%d/%Y\")\n",
    "AP_account = 25702400  # the account from which the money will flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_exchange_rates():\n",
    "    # https://openexchangerates.org\n",
    "    exchange_rates_api_key = \"11f20df062814531be891cc0173702a6\"\n",
    "    api_call = f\"https://openexchangerates.org/api/latest.json?app_id={exchange_rates_api_key}\"\n",
    "    rates_api_response = urlopen(api_call)\n",
    "    rates_api_response_str = rates_api_response.read().decode(\"ascii\")\n",
    "    rates_api_response_dict = loads(rates_api_response_str)\n",
    "    rates = rates_api_response_dict[\"rates\"]\n",
    "\n",
    "    # feel free to update company codes/currencies\n",
    "    currencies_in_scope = {\"AUSA\": \"AUD\", \"BESA\": \"EUR\", \"BGSA\": \"BGN\", \"BRSA\": \"BRL\", \"CASA\": \"CAD\", \"CHSD\": \"CHF\", \"CNSA\": \"CNY\",\n",
    "                           \"CRSB\": \"CRC\", \"CZSA\": \"CZK\", \"DESA\": \"EUR\", \"DKSA\": \"DKK\", \"ESSA\": \"EUR\", \"FRSA\": \"EUR\", \"GBF0\": \"USD\",\n",
    "                           \"GBSA\": \"GBP\", \"IESA\": \"EUR\", \"IESB\": \"EUR\", \"ILSA\": \"ILS\", \"ILSB\": \"ILS\", \"INSA\": \"INR\", \"INSB\": \"INR\",\n",
    "                           \"INSD\": \"INR\", \"ITSA\": \"EUR\", \"JPSA\": \"JPY\", \"LUSB\": \"EUR\", \"MXSC\": \"MXN\", \"NLSC\": \"EUR\", \"PHSB\": \"PHP\",\n",
    "                           \"PLSA\": \"PLN\", \"PRSA\": \"PYG\", \"ROSA\": \"RON\", \"RUSA\": \"RUB\", \"SESA\": \"SEK\", \"TRSA\": \"TRY\", \"USMS\": \"USD\",\n",
    "                           \"USSM\": \"USD\", \"USSN\": \"USD\"}\n",
    "\n",
    "    exchange_rates_to_usd = {}\n",
    "    for company_code in currencies_in_scope:\n",
    "        currency = currencies_in_scope[company_code]\n",
    "        # the rates from API are from USD to x; we need from x to USD\n",
    "        try:\n",
    "            exchange_rate_to_usd = 1/rates[currency]\n",
    "        except:\n",
    "            continue\n",
    "        if company_code in exchange_rates_to_usd:\n",
    "            continue\n",
    "        else:\n",
    "            exchange_rates_to_usd[company_code] = exchange_rate_to_usd\n",
    "    return exchange_rates_to_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_csv(cc):  # cc = Company Code\n",
    "\n",
    "    CSV_file_name = \"../Script/Output/to_upload/{}_Accrual_WD_{}.csv\".format(cc, date_cut)\n",
    "    JE_csv = pd.DataFrame(columns=JE_csv_columns)  # TODO make one template DF and use it for all ccs\n",
    "    cur_cc_data = grouped_by_cc.get_group(cc)\n",
    "    grouped_by_checksum = cur_cc_data.groupby([\"Checksum\"])\n",
    "    posting_month = last_day_of_previous_month.strftime(\"%b\")\n",
    "    posting_year = last_day_of_previous_month.strftime(\"%Y\")\n",
    "    posting_period = \"{} {}\".format(posting_month, posting_year)\n",
    "    # this is a way to track row number, so that groups can be input to consecutive rows\n",
    "    cur_group_start_row = 0\n",
    "\n",
    "    for checksum, g in grouped_by_checksum:\n",
    "        business_area = checksum[-4:]  # BA is the last 4 chars of checksum\n",
    "        profit_center = checksum[:5]  # PC is the first 5 chars of checksum\n",
    "        general_description = \"WD {} ACCRUALS {} FY{}\".format(cc, posting_month, posting_year)\n",
    "        for i in range(cur_group_start_row, cur_group_start_row + len(g)):\n",
    "            # for each line for a given checksum (BA and PC combination), retrieve its Acc# culumn value and input it\n",
    "            # into the next free cell in the \"ACCOUNT\" column in the JE csv form\n",
    "            JE_csv.loc[i, \"ACCOUNT\"] = g.iloc[i - cur_group_start_row][\"Account\"]\n",
    "            JE_csv.loc[i, \"DEBIT\"] = g.iloc[i - cur_group_start_row][\"Net Amount LC\"]\n",
    "            JE_csv.loc[i, \"LINE MEMO\"] = g.iloc[i - cur_group_start_row][\"Expense Report Number\"] + \" Accrual\"\n",
    "            # Note that even though the template has a TRANSACTION DATE - DAY field, it still passes the whole date in mm/dd/YYYY format\n",
    "            JE_csv.loc[i, \"DATE\"] = last_day_of_previous_month.strftime(\"%m/%d/%Y\")\n",
    "            JE_csv.loc[i, \"POSTING PERIOD\"] = posting_period\n",
    "            JE_csv.loc[i, \"SUBSIDIARY\"] = cc\n",
    "            JE_csv.loc[i, \"MEMO\"] = general_description\n",
    "            JE_csv.loc[i, \"REVERSAL DATE\"] = first_day_of_current_month\n",
    "            JE_csv.loc[i, \"MRU\"] = g.iloc[i - cur_group_start_row][\"MRU\"]\n",
    "            JE_csv.loc[i, \"FUNCTIONAL AREA\"] = g.iloc[i - cur_group_start_row][\"Functional Area\"]\n",
    "\n",
    "        # here we're filling out the AP account row\n",
    "        last_group_start_row = cur_group_start_row\n",
    "        cur_group_start_row += len(g)\n",
    "        JE_csv.loc[cur_group_start_row, \"ACCOUNT\"] = AP_account\n",
    "        JE_csv.loc[cur_group_start_row, \"CREDIT\"] = JE_csv.loc[last_group_start_row:cur_group_start_row, \"DEBIT\"].sum()\n",
    "        JE_csv.loc[cur_group_start_row, \"LINE MEMO\"] = general_description\n",
    "        JE_csv.loc[cur_group_start_row, \"BUSINESS AREA\"] = business_area\n",
    "        JE_csv.loc[cur_group_start_row, \"PROFIT CENTER\"] = profit_center\n",
    "        JE_csv.loc[cur_group_start_row, \"DATE\"] = last_day_of_previous_month.strftime(\"%m/%d/%Y\")\n",
    "        JE_csv.loc[cur_group_start_row, \"POSTING PERIOD\"] = posting_period\n",
    "        JE_csv.loc[cur_group_start_row, \"SUBSIDIARY\"] = cc\n",
    "        JE_csv.loc[cur_group_start_row, \"MEMO\"] = general_description\n",
    "        JE_csv.loc[cur_group_start_row, \"REVERSAL DATE\"] = first_day_of_current_month\n",
    "        cur_group_start_row += 1\n",
    "\n",
    "        # TODO: final result should be grouped by sum per report number\n",
    "        # cur_checksum_data = g\n",
    "        # grouped_by_expense_report = cur_checksum_data.groupby([\"Expense Report Number\"])\n",
    "        # report_sum_by_expense_report = grouped_by_expense_report.sum()\n",
    "\n",
    "    JE_amount_local = JE_csv[\"CREDIT\"].sum(skipna=True)\n",
    "    exchange_rates = generate_exchange_rates()\n",
    "    amount_in_usd =  JE_amount_local * exchange_rates[cc]\n",
    "    to_generate = []\n",
    "\n",
    "    # company requirement\n",
    "    if amount_in_usd > 5000:\n",
    "        to_generate.append(cc)\n",
    "\n",
    "    if cc in to_generate:\n",
    "        JE_csv.to_csv(CSV_file_name, index=False)\n",
    "        print(\"{} CSV file generated :)\".format(cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESA CSV file generated :)\n",
      "MXSC CSV file generated :)\n",
      "ROSA CSV file generated :)\n",
      "USSN CSV file generated :)\n"
     ]
    }
   ],
   "source": [
    "for key, group in grouped_by_cc:\n",
    "    company_code = key\n",
    "    generate_csv(company_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for adding card expenses: first concatenate the two dataframes, then run the generate_csv() almost as is:\n",
    "   #### -> remove unnecessary columns fom WD2_report, add some if needed (e.g. Account), and rename the other ones to the same names as in WD1\n",
    "   #### -> WD_report = WD_report.append(WD2_report)\n",
    "   #### -> make sure that for card expenses, line memo includes transaction number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
